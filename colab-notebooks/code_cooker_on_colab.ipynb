{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karaage0703/code-cooker/blob/main/colab-notebooks/code_cooker_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GpLLxVccgsl"
      },
      "source": [
        "# Code Cooker\n",
        "\n",
        "様々なLLMでデータ分析ができるノートブックです。\n",
        "\n",
        "\n",
        "Simple data analysis notebook on google colab like Advanced Data Analysis(Code Interpreter).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic setup"
      ],
      "metadata": {
        "id": "WvJad_iU7NE9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJUYop_25reV"
      },
      "source": [
        "Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTMtDp_9cdGE"
      },
      "outputs": [],
      "source": [
        "!pip -qqq install japanize-matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional"
      ],
      "metadata": {
        "id": "wTDGUl530Dvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qqq install qrcode"
      ],
      "metadata": {
        "id": "N5xK7tZg0DQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oEnT_tb21y2"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdDjVoWN1ijn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import random\n",
        "import time\n",
        "from IPython.core.magic import register_cell_magic\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "\n",
        "import io\n",
        "from contextlib import redirect_stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpfqwwa8JRn_"
      },
      "source": [
        "## Select AI\n",
        "\n",
        "Select AI type and execute AI setup code which you select."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evm9niLMJTg-"
      },
      "outputs": [],
      "source": [
        "AI_TYPE = 'ANTHROPIC' #@param [\"ANTHROPIC\", \"OPENAI\", \"PHI3\", \"LLAMA3\"] {allow-input: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H2TnDqG1qkw"
      },
      "source": [
        "### Anthropic setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su23quzQ1xtm"
      },
      "outputs": [],
      "source": [
        "!pip -qqq install anthropic\n",
        "\n",
        "import anthropic\n",
        "\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")\n",
        "client_anthropic = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
        "\n",
        "def chat_claude(prompt_w_history, system_prompt):\n",
        "    print(\"I am Claude\")\n",
        "    response = client_anthropic.messages.create(\n",
        "        max_tokens=2048,\n",
        "        messages=prompt_w_history,\n",
        "        model=\"claude-3-opus-20240229\",\n",
        "        system=system_prompt\n",
        "    )\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isqphpGB1yCZ"
      },
      "source": [
        "### OpenAI setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9hyd2OHi8bh"
      },
      "outputs": [],
      "source": [
        "!pip -qqq install openai\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "client_openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "def chat_gpt(prompt_w_history):\n",
        "    print(\"I am GPT\")\n",
        "    response = client_openai.chat.completions.create(\n",
        "        model=\"gpt-4-turbo-2024-04-09\",\n",
        "        messages=prompt_w_history,\n",
        "    )\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGmHl6Da42Bz"
      },
      "source": [
        "### Phi-3 setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCUOU1IE46rH"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq transformers accelerate\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-128k-instruct\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "\n",
        "def chat_phi3(prompt_w_history):\n",
        "    print(\"I am Phi-3\")\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(prompt_w_history, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # 推論の実行\n",
        "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            token_ids.to(model.device),\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            max_new_tokens=256,\n",
        "        )\n",
        "    response = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbg9G1AAAPes"
      },
      "source": [
        "### Llama3 setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUfoMpvxChnI"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq transformers accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "login(token = userdata.get('HF_TOKEN'))\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        ")\n",
        "\n",
        "\n",
        "def chat_llama3(prompt_w_history):\n",
        "    print(\"I am Llama 3\")\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(prompt_w_history, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            token_ids.to(model.device),\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            top_p=0.9,\n",
        "            max_new_tokens=256,\n",
        "            eos_token_id=[\n",
        "                tokenizer.eos_token_id,\n",
        "                tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "            ],\n",
        "        )\n",
        "    response = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFfvnNupCKoJ"
      },
      "source": [
        "## Code cook\n",
        "Define function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcgU0Dfe_F7l"
      },
      "outputs": [],
      "source": [
        "MAX_RETRAY_COUNT = 3\n",
        "\n",
        "@register_cell_magic\n",
        "def _(line, cell):\n",
        "    system_prompt = f\"\"\"あなたはJupyterNotebookを使う優秀なPythonデータサイエンティストです。\n",
        "    入力された指示をもとに、以下の要件に従ってデータ分析を実施してください。\n",
        "\n",
        "    ###\n",
        "    - 回答には、1つのPythonコードブロック（マジックコマンド、クラス定義、コンストラクタを含む）のみを含め、最初を```pythonで最後を```で囲ってください。\n",
        "    - あなたは新しいPythonライブラリを自分で新たにインストールすることはできないので、ライブラリの不足時は、ユーザーにインストールする方法を提示してください。\n",
        "    - 中身の分からないデータを操作するときは、まずはデータを確認するために読み込んだデータの情報(info, head)を出力するところまでだけを回答して。もう一度実行するようにユーザーにうながして下さい。\n",
        "    - プロンプトにデータの情報が含まれていたら、その情報をもとに処理をすすめてください。\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = cell\n",
        "    filename = line\n",
        "\n",
        "    if filename != \"\":\n",
        "        system_prompt += f\"\"\"\n",
        "        # 入力データ:ファイルパス\n",
        "        {filename}\n",
        "        なお、データはUTF-8で読み込んでください。\n",
        "        \"\"\"\n",
        "\n",
        "    if AI_TYPE == \"OPENAI\":\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "    if AI_TYPE == \"ANTHROPIC\":\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "    if AI_TYPE == \"PHI3\":\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "         ]\n",
        "    if AI_TYPE == \"LLAMA3\":\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "         ]\n",
        "\n",
        "    prompt_w_history.extend(messages)\n",
        "\n",
        "    retry_count = 0\n",
        "    while retry_count < MAX_RETRAY_COUNT:\n",
        "        try:\n",
        "            if AI_TYPE == \"OPENAI\":\n",
        "                response = chat_gpt(prompt_w_history)\n",
        "            if AI_TYPE == \"ANTHROPIC\":\n",
        "                response = chat_claude(prompt_w_history, system_prompt)\n",
        "            if AI_TYPE == \"PHI3\":\n",
        "                response = chat_phi3(prompt_w_history)\n",
        "            if AI_TYPE == \"LLAMA3\":\n",
        "                response = chat_llama3(prompt_w_history)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(3)\n",
        "            continue\n",
        "\n",
        "        if AI_TYPE == \"OPENAI\":\n",
        "            assistant_response = response.choices[0].message.content\n",
        "        if AI_TYPE == \"ANTHROPIC\":\n",
        "            assistant_response = response.content[0].text\n",
        "        if AI_TYPE == \"PHI3\":\n",
        "            assistant_response = response\n",
        "        if AI_TYPE == \"LLAMA3\":\n",
        "            assistant_response = response\n",
        "\n",
        "        print(\"APIからの応答:\", assistant_response)\n",
        "\n",
        "        python = \"\"\n",
        "        if \"```python\" in assistant_response and \"```\" in assistant_response:\n",
        "            python = assistant_response.split(\"```python\")[1].split(\"```\")[0]\n",
        "\n",
        "        # print(\"抽出されたPythonコード:\", python)\n",
        "\n",
        "        try:\n",
        "            stream = io.StringIO()\n",
        "\n",
        "            with redirect_stdout(stream):\n",
        "                exec(python)\n",
        "\n",
        "            output_stream = stream.getvalue()\n",
        "\n",
        "            print(\"Result:\")\n",
        "            print(output_stream)\n",
        "\n",
        "            prompt_w_history.append({\"role\": \"assistant\", \"content\": str(assistant_response) + 'result:' + output_stream})\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"エラーが発生しました: {e}\")\n",
        "            prompt_w_history.append({\"role\": \"assistant\", \"content\": str(assistant_response)})\n",
        "            retry_count += 1\n",
        "            if retry_count < MAX_RETRAY_COUNT:\n",
        "                prompt_w_history.append({\"role\": \"user\", \"content\": f\"エラーが発生しました。エラーメッセージ: {e}\\nコードを修正して再度実行してください。\"})\n",
        "\n",
        "    print(\"コードの実行が完了しました。\" if retry_count < MAX_RETRAY_COUNT else \"コードの修正に失敗しました。再実行回数が上限に達しました。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsq41P6Z_wqY"
      },
      "source": [
        "Initialize prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEdJlSE2_vHe"
      },
      "outputs": [],
      "source": [
        "prompt_w_history = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLVHtv31CUBv"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjfe00cD9COT"
      },
      "outputs": [],
      "source": [
        "%%_ sample_excel.xlsx\n",
        "アップロードしたExcelファイルの金額を月ごとに集計して棒グラフにしてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgP5cUAjF1Pq"
      },
      "outputs": [],
      "source": [
        "%%_\n",
        "架空の国の地域ごとの人口比率のデータを可視化してください。日本語でお願いします。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference:\n",
        "- [claude_manim.ipynb](https://colab.research.google.com/github/wmoto-ai/claude-manim-notebook/blob/main/claude_manim.ipynb#scrollTo=mF8X8Q3LyZc9)\n",
        "- [Google Colab で Llama 3 を試す](https://note.com/npaka/n/n73b0786f48e9)\n",
        "- [Google Colab で Phi-3 を試す](https://note.com/npaka/n/n9915dbd44a84)"
      ],
      "metadata": {
        "id": "htHvpjqWGh9k"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNPJ7TSqSc1olSaSHAvN/X+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}