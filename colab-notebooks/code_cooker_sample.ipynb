{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karaage0703/code-cooker/blob/main/colab-notebooks/code_cooker_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GpLLxVccgsl"
      },
      "source": [
        "# Code Cooker\n",
        "\n",
        "様々なLLMでデータ分析ができるノートブックです。\n",
        "\n",
        "\n",
        "Simple data analysis notebook on google colab like Advanced Data Analysis(Code Interpreter).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic setup"
      ],
      "metadata": {
        "id": "WvJad_iU7NE9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJUYop_25reV"
      },
      "source": [
        "ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTMtDp_9cdGE"
      },
      "outputs": [],
      "source": [
        "!pip -qqq install japanize-matplotlib\n",
        "!pip -qqq install qrcode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oEnT_tb21y2"
      },
      "source": [
        "ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdDjVoWN1ijn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import random\n",
        "import time\n",
        "from IPython.core.magic import register_cell_magic\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "\n",
        "import io\n",
        "from contextlib import redirect_stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "システムプロンプトの設定"
      ],
      "metadata": {
        "id": "OVxt78C-kpVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\n",
        "    あなたはJupyterNotebookを使う優秀なPythonデータサイエンティストです。\n",
        "    入力された指示をもとに、以下の要件に従ってデータ分析を実施してください。\n",
        "\n",
        "    ###\n",
        "    - 回答には、1つのPythonコードブロック（マジックコマンド、クラス定義、コンストラクタを含む）のみを含め、最初を```pythonで最後を```で囲ってください。\n",
        "    - あなたは新しいPythonライブラリを自分で新たにインストールすることはできないので、ライブラリの不足時は、ユーザーにインストールする方法を提示してください。\n",
        "    - 中身の分からないデータを操作するときは、まずはデータを確認するために読み込んだデータの情報(info, head)を出力するところまでだけを回答して。もう一度実行するようにユーザーにうながして下さい。\n",
        "    - プロンプトにデータの情報が含まれていたら、その情報をもとに処理をすすめてください。\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "QjtnnkdqkfL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpfqwwa8JRn_"
      },
      "source": [
        "## Select AI\n",
        "\n",
        "Select AI type and execute AI setup code which you select."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evm9niLMJTg-"
      },
      "outputs": [],
      "source": [
        "AI_TYPE = 'OPENAI' #@param [\"OPENAI\", \"ANTHROPIC\", \"GEMINI\", \"PHI3\", \"LLAMA3\"] {allow-input: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isqphpGB1yCZ"
      },
      "source": [
        "### OpenAI GPT-4o setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9hyd2OHi8bh"
      },
      "outputs": [],
      "source": [
        "!pip -qqq install openai\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "client_openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "def chat_gpt(prompt_w_history):\n",
        "    print(\"I am GPT\")\n",
        "    response = client_openai.chat.completions.create(\n",
        "        model=\"gpt-4o-2024-08-06\",\n",
        "        messages=prompt_w_history,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H2TnDqG1qkw"
      },
      "source": [
        "### Anthropic Claude 3.5 Sonnet setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su23quzQ1xtm"
      },
      "outputs": [],
      "source": [
        "!pip -qqq install anthropic\n",
        "\n",
        "import anthropic\n",
        "\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")\n",
        "client_anthropic = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
        "\n",
        "def chat_claude(prompt_w_history, system_prompt):\n",
        "    print(\"I am Claude\")\n",
        "    response = client_anthropic.messages.create(\n",
        "        max_tokens=2048,\n",
        "        messages=prompt_w_history,\n",
        "        model=\"claude-3-5-sonnet-20240620\",\n",
        "        system=system_prompt\n",
        "    )\n",
        "\n",
        "    return response.content[0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Gemini 1.5 Pro setup"
      ],
      "metadata": {
        "id": "QkSmBvx5iw_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qqq install google-generativeai\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "genai.configure(\n",
        "    api_key=os.getenv('GEMINI_API_KEY')\n",
        ")\n",
        "\n",
        "client_google = genai.GenerativeModel('gemini-1.5-pro-latest', system_instruction=[system_prompt])\n",
        "\n",
        "def chat_gemini(prompt_w_history):\n",
        "    print(\"I am Gemini\")\n",
        "    response = client_google.generate_content(\n",
        "        prompt_w_history\n",
        "    )\n",
        "\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "bNzay9HfivgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGmHl6Da42Bz"
      },
      "source": [
        "### Microsoft Phi-3 setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCUOU1IE46rH"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq transformers accelerate\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-128k-instruct\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "\n",
        "def chat_phi3(prompt_w_history):\n",
        "    print(\"I am Phi-3\")\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(prompt_w_history, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # 推論の実行\n",
        "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            token_ids.to(model.device),\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            max_new_tokens=256,\n",
        "        )\n",
        "    response = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbg9G1AAAPes"
      },
      "source": [
        "### Meta Llama3 setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUfoMpvxChnI"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq transformers accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "login(token = userdata.get('HF_TOKEN'))\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        ")\n",
        "\n",
        "\n",
        "def chat_llama3(prompt_w_history):\n",
        "    print(\"I am Llama 3\")\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(prompt_w_history, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            token_ids.to(model.device),\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            top_p=0.9,\n",
        "            max_new_tokens=256,\n",
        "            eos_token_id=[\n",
        "                tokenizer.eos_token_id,\n",
        "                tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "            ],\n",
        "        )\n",
        "    response = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFfvnNupCKoJ"
      },
      "source": [
        "## Code cook\n",
        "関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcgU0Dfe_F7l"
      },
      "outputs": [],
      "source": [
        "MAX_RETRAY_COUNT = 3\n",
        "\n",
        "@register_cell_magic\n",
        "def _(line, cell):\n",
        "    global system_prompt\n",
        "\n",
        "    user_prompt = cell\n",
        "    filename = line\n",
        "\n",
        "    if filename != \"\":\n",
        "        system_prompt += f\"\"\"\n",
        "        # 入力データ:ファイルパス\n",
        "        {filename}\n",
        "        なお、データはUTF-8で読み込んでください。\n",
        "        \"\"\"\n",
        "\n",
        "    if AI_TYPE == \"OPENAI\":\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "    if AI_TYPE == \"ANTHROPIC\":\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "    if AI_TYPE == \"GEMINI\":\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"parts\": [user_prompt]},\n",
        "        ]\n",
        "    if AI_TYPE == \"PHI3\":\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "         ]\n",
        "    if AI_TYPE == \"LLAMA3\":\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "         ]\n",
        "\n",
        "    prompt_w_history.extend(messages)\n",
        "\n",
        "    retry_count = 0\n",
        "    while retry_count < MAX_RETRAY_COUNT:\n",
        "        try:\n",
        "            if AI_TYPE == \"OPENAI\":\n",
        "                assistant_response = chat_gpt(prompt_w_history)\n",
        "            if AI_TYPE == \"ANTHROPIC\":\n",
        "                assistant_response = chat_claude(prompt_w_history, system_prompt)\n",
        "            if AI_TYPE == \"GEMINI\":\n",
        "                assistant_response = chat_gemini(prompt_w_history)\n",
        "            if AI_TYPE == \"PHI3\":\n",
        "                assistant_response = chat_phi3(prompt_w_history)\n",
        "            if AI_TYPE == \"LLAMA3\":\n",
        "                assistant_response = chat_llama3(prompt_w_history)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(3)\n",
        "            continue\n",
        "\n",
        "        print(\"APIからの応答:\", assistant_response)\n",
        "\n",
        "        # Pythonコードの抽出\n",
        "        python = \"\"\n",
        "        if \"```python\" in assistant_response and \"```\" in assistant_response:\n",
        "            python = assistant_response.split(\"```python\")[1].split(\"```\")[0]\n",
        "\n",
        "        # print(\"抽出されたPythonコード:\", python)\n",
        "\n",
        "        try:\n",
        "            stream = io.StringIO()\n",
        "\n",
        "            with redirect_stdout(stream):\n",
        "                exec(python)\n",
        "\n",
        "            output_stream = stream.getvalue()\n",
        "\n",
        "            print(\"Result:\")\n",
        "            print(output_stream)\n",
        "\n",
        "            # プロンプトに追加\n",
        "            if AI_TYPE == \"GEMINI\":\n",
        "                prompt_w_history.append({\"role\": \"model\", \"parts\": [str(assistant_response)]})\n",
        "            else:\n",
        "                prompt_w_history.append({\"role\": \"assistant\", \"content\": str(assistant_response) + 'result:' + output_stream})\n",
        "            break\n",
        "        except Exception as e:\n",
        "            # エラー処理\n",
        "            print(f\"エラーが発生しました: {e}\")\n",
        "            if AI_TYPE == \"GEMINI\":\n",
        "                prompt_w_history.append({\"role\": \"model\", \"parts\": str(assistant_response)})\n",
        "            else:\n",
        "                prompt_w_history.append({\"role\": \"assistant\", \"content\": str(assistant_response)})\n",
        "            retry_count += 1\n",
        "            if retry_count < MAX_RETRAY_COUNT:\n",
        "                # エラー内容をプロンプトに追加\n",
        "                if AI_TYPE == \"GEMINI\":\n",
        "                    prompt_w_history.append({\"role\": \"user\", \"parts\": [f\"エラーが発生しました。エラーメッセージ: {e}\\nコードを修正して再度実行してください。\"]})\n",
        "                else:\n",
        "                    prompt_w_history.append({\"role\": \"user\", \"content\": f\"エラーが発生しました。エラーメッセージ: {e}\\nコードを修正して再度実行してください。\"})\n",
        "\n",
        "    print(\"コードの実行が完了しました。\" if retry_count < MAX_RETRAY_COUNT else \"コードの修正に失敗しました。再実行回数が上限に達しました。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsq41P6Z_wqY"
      },
      "source": [
        "プロンプトの初期化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEdJlSE2_vHe"
      },
      "outputs": [],
      "source": [
        "prompt_w_history = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLVHtv31CUBv"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgP5cUAjF1Pq"
      },
      "outputs": [],
      "source": [
        "%%_\n",
        "架空の国の地域ごとの人口比率のデータを可視化してください。日本語でお願いします。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%_\n",
        "架空の国の地域ごとの人口比率のデータを可視化してください。データはないので適当でOKです。\n",
        "日本語でお願いします。"
      ],
      "metadata": {
        "id": "1ek2VzEAr_dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjfe00cD9COT"
      },
      "outputs": [],
      "source": [
        "%%_ sample_excel.xlsx\n",
        "アップロードしたExcelファイルの金額を月ごとに集計して棒グラフにしてください。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%_\n",
        "以下のURLをQRコードにしてください。\n",
        "\n",
        "https://karaage.hatenadiary.jp/"
      ],
      "metadata": {
        "id": "0kk3y2bEiG7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%_ soraumineko_01.jpg\n",
        "画像をモノクロにして表示してください。"
      ],
      "metadata": {
        "id": "prajPf6Tibwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%_ face_sample.jpg\n",
        "画像に対してdlibライブラリを使った顔検出をして表示してください。\n",
        "「import dlib」でdlibライブラリを使用できます。"
      ],
      "metadata": {
        "id": "A7YUab_SvW5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%_ newly_confirmed_cases_daily_japanese.csv\n",
        "これは、新型コロナウイルスの全国と都道府県別での新規陽性者数のデータです。\n",
        "このデータを可視化します。まず、全国の新規陽性者数の折れ線グラフを描いて表示してください。"
      ],
      "metadata": {
        "id": "vzLtEg4LwCT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%_ newly_confirmed_cases_daily_japanese.csv\n",
        "東京都、大阪府、愛知県、福岡県の新規陽性者数について、折れ線グラフを描いて表示してください。\n",
        "データは2022年1月以降のものを使ってください。"
      ],
      "metadata": {
        "id": "D6q_GUHYwOhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%_ newly_confirmed_cases_daily_japanese.csv\n",
        "東京都、大阪府、愛知県、福岡県の新規陽性者数について、7日移動平均のグラフを描いて表示してください。\n",
        "データは2022年1月以降のものを使ってください。"
      ],
      "metadata": {
        "id": "Hrwu3HMwwWqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%_ newly_confirmed_cases_daily_japanese.csv\n",
        "要件に従って、アニメーションGIFファイル 'anime.gif' を作成してください。\n",
        "\n",
        "要件\"\"\"\n",
        "・Pillowを使う。\n",
        "・東京都、大阪府、愛知県、福岡県の新規陽性者について、7日移動平均のグラフを描く。\n",
        "・全部で10コマのアニメーションにする。\n",
        "・x軸を固定し、7日移動平均のグラフが徐々に右に伸びていくようにする。\n",
        "・y軸を固定する。\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "c1587cg5wiHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "out_filename = 'anime.gif'\n",
        "IPython.display.Image(out_filename, format='png')"
      ],
      "metadata": {
        "id": "-XIAjGo4xDoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference:\n",
        "- [claude_manim.ipynb](https://colab.research.google.com/github/wmoto-ai/claude-manim-notebook/blob/main/claude_manim.ipynb#scrollTo=mF8X8Q3LyZc9)\n",
        "- [Google Colab で Llama 3 を試す](https://note.com/npaka/n/n73b0786f48e9)\n",
        "- [Google Colab で Phi-3 を試す](https://note.com/npaka/n/n9915dbd44a84)\n",
        "- [Google Colab上で手軽にAdvanced Data Analysis(Code Interpreter)的機能を実現する方法](https://zenn.dev/karaage0703/articles/f6a1df0b2eabf4)\n",
        "- [LLMに面倒なことをやらせるソフト「Code Cooker」の紹介](https://zenn.dev/karaage0703/articles/8996f211fc38cd)"
      ],
      "metadata": {
        "id": "htHvpjqWGh9k"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "MGmHl6Da42Bz",
        "Qbg9G1AAAPes"
      ],
      "authorship_tag": "ABX9TyNWj6Nqh04z4XNCYXg83NdZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}